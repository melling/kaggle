---
title: "TabulaR Playground: EDA"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<center>
![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Multiplication_table_to_scale.svg/1200px-Multiplication_table_to_scale.svg.png){width=40%}
</center>

# Introduction
This is a starter notebook for the Tabular Playground Series - Jan 2021 competition with R language. The main goal of the kernel is to present steps needed for building a model using [R](https://www.r-project.org/), [tidyverse](https://www.tidyverse.org/), and, possibly, [torch](https://torch.mlverse.org/) and [lightgbm](https://cran.r-project.org/web/packages/lightgbm/index.html) packages.

This is a supervised machine learning problem which is evaluated on [the root mean squared error](https://en.wikipedia.org/wiki/Root-mean-square_deviation):
$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})}
$$

# Preparations {.tabset .tabset-fade}
## Libraries
```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(umap)
library(knitr)
library(GGally)
library(mclust)
library(recipes)
library(rsample)
library(lightgbm)
library(tidyverse)
library(ggcorrplot)
```
## Constants
```{r set_const, message=FALSE, warning=FALSE, results='hide'}
set.seed(0)

path <- "../input/tabular-playground-series-jan-2021/"
path <- "./"
kfolds <- 5
```
## Load Data
```{r load_tab, message=FALSE, warning=FALSE, results='hide'}
tr <- read_csv(str_c(path, "train.csv"))
te <- read_csv(str_c(path, "test.csv"))
sub <- read_csv(str_c(path, "sample_submission.csv"))
```

# Tabular Overview {.tabset .tabset-fade}
## Train
```{r tr_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(tr, 5) %>% kable()
```
## Test
```{r te_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(te, 5) %>% kable()
```
## Submission
```{r sub_csv, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
head(sub, 5) %>% kable()
```
<div></div>

# Train & Test Set
## Glimpse at the dataset
Let's have a closer look at the training set:
```{r tr1, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
glimpse(tr)
```
In total we have 16 columns:

* An **id** column
* 14 continuous feature columns
* A **target** column

The test set has the same columns except for the **target**, which we have to predict.

## Distributions: Train vs Test
```{r tr_fea_dist, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.height=12, fig.align='center'}
tr %>% 
  select(starts_with("cont")) %>% 
  mutate(grp = "train") %>%
  bind_rows(
    (te %>% 
       select(starts_with("cont")) %>% 
       mutate(grp = "test"))
  ) %>% 
  pivot_longer(cols = starts_with("cont")) %>% 
  group_by(name, grp) %>% 
  mutate(mean = mean(value)) %>% 
  ggplot(aes(x = value)) + 
  facet_wrap(~name, ncol = 2, scales = "free") +
  geom_density(aes(fill = grp), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.2) +
  theme_minimal() + 
  theme(legend.position = "top") +
  labs(fill = "")
```

* Both train and test distributions are identical
* We might hope that our model will work correctly with the test set

## Correlations
In the plot below we might see that no feature has significant correlation with the **target**. Several features correlate with each other, e.g.,  **cont6** and **cont10**, **cont11** and **cont12**. We might try to remove some highly correlated variables (actually, I tried and it didn't imrpode the score).
```{r tr_corr, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.height=6, fig.align='center'}
tr %>% 
  select(-id) %>% 
  ggcorr(label = TRUE, label_size = 3, label_round = 2, label_alpha = TRUE)
```

It's strange that almost nothing correlates with the target. The pairs plot below show realtions between features themselves:
```{r tr_corr2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.width=9, fig.height=9, fig.align='center'}
tr %>% 
  select(-id, -target) %>% 
  sample_frac(0.05) %>% 
  ggpairs(lower = list(continuous = wrap(ggally_points, col = "steelblue", size = 0.0025)),
          diag = list(continuous = wrap(ggally_densityDiag, col = "#F8766D")),
          upper = list(continuous = "blank"), 
          axisLabels = "none",
          progress = FALSE) +
  theme_minimal()
```

You might have noticed the strange pattern of the `cont2` feature:
```{r tr_fea_dist_cont2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
tr %>% 
  select(cont2) %>% 
  mutate(mean = mean(cont2)) %>% 
  ggplot(aes(x = cont2)) + 
  geom_histogram(alpha = 0.8, bins = 500) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.2) +
  theme_minimal() + 
  theme(legend.position = "top") +
  labs(fill = "", y = "")
```

It looks like a bunch of normal distributions. It might be that all these features were generated as sums of normal distributions - there are many multimodal features.

## UMAP
It's interesting what we can obtain using UMAP to visualize the data.
```{r umap1, message=FALSE, warning=FALSE, echo=TRUE, results='show'}
tb <- tr %>% 
  select(-id) %>% 
  sample_frac(0.2)

m_umap <- tb %>% 
  select(-target) %>% 
  umap(method = "naive", n_epochs = 25, n_neighbors = 50, n_components = 3)

tb <- bind_cols(tb, x = m_umap$layout[, 1], y = m_umap$layout[, 2], z = m_umap$layout[, 3]) 
```
<div align="center">
```{r umap2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.height=6, fig.width=6, fig.align='center'}
plotly::plot_ly(tb, x = ~x, y = ~y, z = ~z, color = ~cut(tb$target, 9), alpha = 0.9, size = 0.1)
```
</div>

Though we might see some clusters we can't group points by their target values, thus low-dimensional representation is quite useless. 

## Target 
```{r tr_tar1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
summary(tr$target)
```

```{r tr_tar2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
tr %>% 
  select(target) %>% 
  mutate(log_target = log1p(target)) %>% 
  pivot_longer(cols = everything()) %>% 
  group_by(name) %>% 
  mutate(mean = mean(value)) %>% 
  ggplot(aes(x = value)) +
  facet_wrap(~ name, ncol = 2, scales = "free") +
  geom_density(aes(fill=name), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.2) +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs(fill = "", x = "", y = "")
```

* Distribution of the **target** column is bimodal 
* This is a left-skewed distribution
* Log-transform of the **target** column doesn't help a lot
* Previously I mentioned that in the dataset there are multimodal features - their mixture leads to multimodal target
* The distribution looks like the mixture of regressions

I use the `mclustBIC()` function from the [mclust package](https://cran.r-project.org/web/packages/mclust/) to estimate the number of mixture components:
```{r mix1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
m_bic <- mclustBIC(tr$target)
summary(m_bic)
plot(m_bic)
```

From the plot above we can assume that there might be more than 2 clusters. To start, let's assume that there are two components. I use the `Mclust()` function to classify the components of the mixture:
```{r mix2, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
m_mix <- mclust::Mclust(tr$target, 2)

tr %>% 
  mutate(class = factor(m_mix$classification),
         mean = m_mix$parameters$mean[class]) %>% 
  ggplot(aes(x = target)) +
  geom_density(aes(fill = class), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.3) +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs(fill = "", x = "", y = "")
```

These two distributions don't look like pure gaussians. If you want to learn more about how to address bimodality of the target, you might view [this notebook](https://www.kaggle.com/docxian/tabular-playground-1-bimodal-two-step-model/comments).

# Data Preprocessing
At this step we use the [recipes](https://cran.r-project.org/web/packages/recipes/index.html) package, which makes data preprocessing much easier. We remove **id** and **target** columns and normalize numeric columns:
```{r pre1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
(rec <- tr %>%
   recipe(~ .) %>%
   step_rm(id, target) %>% 
   step_normalize(all_numeric()) %>%
   prep())
```
Let's prepare the dataset using the recipe:
```{r pre2, message=FALSE, warning=FALSE, results='hide'}
y <- tr$target
X <- juice(rec, composition = "matrix")
X_te <- bake(rec, te, composition = "matrix")
```
As a result we get matrices ready to be fed into a model.

# LightGBM
## Training a GBM Model
Here we do a simple training loop over k-folds:
```{r gbm1, message=FALSE, warning=FALSE, results='show'}
p <- list(objective = "regression", 
          metric = "rmse",
          feature_pre_filter = FALSE,
          learning_rate = 0.0034,
          num_leaves = 116,
          min_child_samples = 64,
          sub_feature = 0.34,
          sub_row = 0.68,
          subsample_freq = 21,
          lambda_l1 = 9.187,
          lambda_l2 = 1.795)

oob <- numeric(nrow(X))
pred <- numeric(nrow(X_te))
imp <- tibble()
scores <- c()
for (rs in vfold_cv(tr, kfolds)$splits) {
  cat("Fold id:", rs$id$id, "\n")
  tri <- rs$in_id
  
  xtr <- lgb.Dataset(X[tri, ], label = y[tri])
  xval <- lgb.Dataset(X[-tri, ], label = y[-tri])
  
  m_lgb <- lgb.train(params = p,
                     data = xtr,
                     nrounds = 30000,
                     valids = list(val = xval),
                     early_stopping_rounds = 300,
                     eval_freq = 300, 
                     verbose = -1)
  
  oob[-tri] <- predict(m_lgb, X[-tri, ])
  pred <- pred + predict(m_lgb, X_te) / kfolds  
  
  imp <- bind_rows(imp, lgb.importance(m_lgb))
  
  scores <- c(scores, m_lgb$best_score)
  cat("\tScore:", tail(scores, 1), "\n")
}
```
```{r gbm2, message=FALSE, warning=FALSE, results='show'}
cat("Mean score:", mean(scores), "\n")
```

## Feature Importance in GBM
```{r gbm3, message=FALSE, warning=FALSE, results='show'}
imp %>% 
  group_by(Feature) %>% 
  summarise(Gain = mean(Gain)) %>% 
  ggplot(aes(reorder(Feature, Gain), Gain)) + 
  geom_col(fill = "steelblue") +
  xlab("Feature") +
  ggtitle("GBM") +
  coord_flip() +
  theme_minimal()
```

## Distribution of Predictions
Let's check how our model predicts train and test data - we are interested in shapes of distributions:
```{r pred_d1, message=FALSE, warning=FALSE, echo=TRUE, results='show', fig.align='center'}
tr %>% 
  select(target) %>% 
  mutate(source = "Original target") %>% 
  bind_rows(tibble(target = oob, source = "Oob")) %>% 
  bind_rows(tibble(target = pred, source = "Predicted test target")) %>% 
  group_by(source) %>% 
  mutate(mean = mean(target)) %>% 
  ggplot(aes(x = target)) +
  geom_density(aes(fill = source), alpha = 0.3) +
  geom_vline(aes(xintercept = mean), linetype = "dashed", size = 0.3) +
  theme_minimal() + 
  ggtitle("Original target and predictions distribution") +
  labs(fill = "", x = "", y = "")
```

We observe that predicted values have a distribution with a shape that is far from the shape of the distribution of the original target, but the mean values coincide. This might be due to the fact that:

1. Target is multimodal (as it was show by the `Mclust` model earlier)
2. Regression trees return the average of training instances in a leaf. 

Thus we might try to train two tree models depending on the the class of the target - or use a quantile regression.

## GBM Submission
```{r sub1, message=FALSE, warning=FALSE, results='show'}
sub %>% 
  mutate(target = pred) %>% 
  write_csv("sub.csv")
```

# Mix of Mixes
Actually, the easiest way to get the highest score on LB is to mix the outputs of other kernels. And in this section we create a submission (seemingly, overfitted) using some random weights. You could easily beat this score having chosen another set of weights while probing the LB:
```{r sub_easy, message=FALSE, warning=FALSE, results='show'}
sub_easy <- read_csv("../input/results-driven-tabular-playground-series-201/submission.csv")
sub %>% 
  mutate(target = 0.01*pred + 0.99*sub_easy$target) %>% 
  write_csv("sub_easy_mix.csv")
```